{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Scientific Analysis: Impact of Neighborhood Size on NCA Models\n",
        "\n",
        "This notebook performs a comprehensive scientific analysis to determine whether it makes sense to use a `neighborhood_size` greater than 3, and what are the differences between models with different neighborhood sizes.\n",
        "\n",
        "## Analysis Objectives:\n",
        "1. **Performance Evaluation**: Comparison of biological metrics across different neighborhood sizes\n",
        "2. **Statistical Tests**: Verification of statistical significance of differences\n",
        "3. **Trend Analysis**: Identification of patterns and improvements/degradations\n",
        "4. **Computational Complexity**: Analysis of computational cost vs. benefits\n",
        "5. **Interactive Visualizations**: Plotly charts for in-depth exploration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Add parent directory to path\n",
        "# Get the directory where this notebook is located\n",
        "notebook_dir = Path().absolute()\n",
        "# Get the project root (parent of notebooks directory)\n",
        "project_root = notebook_dir.parent\n",
        "# Add to path\n",
        "sys.path.insert(0, str(project_root))\n",
        "sys.path.insert(0, str(project_root / 'experiments'))\n",
        "\n",
        "# Import the analyzer\n",
        "from experiments.analyze_neighborhood_sizes import NeighborhoodSizeAnalyzer\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "print(\"Imports completed!\")\n",
        "print(f\"Project root: {project_root}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Define the parameters for the analysis:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "# Use absolute paths based on project root\n",
        "# If running from notebooks/, go up one level to project root\n",
        "if 'notebooks' in str(notebook_dir):\n",
        "    project_root = notebook_dir.parent\n",
        "else:\n",
        "    project_root = notebook_dir\n",
        "\n",
        "RESULTS_DIR = str(project_root / \"experiments\" / \"results_extended\")\n",
        "HISTORIES_PATH = str(project_root / \"histories.npy\")\n",
        "DEVICE = \"auto\"  # \"auto\", \"cuda\", \"mps\", or \"cpu\"\n",
        "N_EVALUATIONS = 10  # Number of evaluations for stochastic models\n",
        "NEIGHBORHOOD_SIZES = [3, 4, 5, 6, 7]\n",
        "FORCE_RECOMPUTE = False  # If True, re-evaluate even if CSV files exist\n",
        "\n",
        "print(f\"Notebook directory: {notebook_dir}\")\n",
        "print(f\"Project root: {project_root}\")\n",
        "print(f\"Results directory: {RESULTS_DIR}\")\n",
        "print(f\"Histories path: {HISTORIES_PATH}\")\n",
        "print(f\"Device: {DEVICE}\")\n",
        "print(f\"Neighborhood sizes: {NEIGHBORHOOD_SIZES}\")\n",
        "print(f\"Number of evaluations: {N_EVALUATIONS}\")\n",
        "print(f\"Paths exist: RESULTS_DIR={os.path.exists(RESULTS_DIR)}, HISTORIES={os.path.exists(HISTORIES_PATH)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyzer Initialization\n",
        "\n",
        "Create the analyzer instance and load/evaluate the models:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the analyzer\n",
        "analyzer = NeighborhoodSizeAnalyzer(\n",
        "    results_dir=RESULTS_DIR,\n",
        "    histories_path=HISTORIES_PATH,\n",
        "    device=DEVICE,\n",
        "    n_evaluations=N_EVALUATIONS\n",
        ")\n",
        "\n",
        "# Load or evaluate the models\n",
        "analyzer.load_or_evaluate_models(\n",
        "    neighborhood_sizes=NEIGHBORHOOD_SIZES,\n",
        "    force_recompute=FORCE_RECOMPUTE\n",
        ")\n",
        "\n",
        "print(\"\\n Models loaded/evaluated successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Exploration\n",
        "\n",
        "Examine the metrics data:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parse the metrics\n",
        "df = analyzer.parse_metrics()\n",
        "\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"\\nColumns:\", df.columns.tolist())\n",
        "print(\"\\nFirst data:\")\n",
        "df.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Descriptive statistics by model and neighborhood size\n",
        "print(\"Descriptive statistics by model:\")\n",
        "print(\"=\"*60)\n",
        "for model_type in df['Model Type'].unique():\n",
        "    print(f\"\\n{model_type}:\")\n",
        "    model_data = df[df['Model Type'] == model_type]\n",
        "    print(model_data.groupby('Neighborhood Size').agg(['mean', 'std']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Statistical Tests\n",
        "\n",
        "Perform statistical tests to verify the significance of differences:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform statistical tests\n",
        "stat_results = analyzer.statistical_tests()\n",
        "\n",
        "# Display results in a more readable format\n",
        "import json\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STATISTICAL TEST RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for metric, model_results in stat_results.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"METRIC: {metric}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    for model_type, results in model_results.items():\n",
        "        if 'kruskal_wallis' in results:\n",
        "            kw = results['kruskal_wallis']\n",
        "            significance = '***' if kw['p_value'] < 0.001 else '**' if kw['p_value'] < 0.01 else '*' if kw['p_value'] < 0.05 else '(not significant)'\n",
        "            print(f\"\\n  {model_type}:\")\n",
        "            print(f\"    Kruskal-Wallis: H={kw['statistic']:.4f}, p={kw['p_value']:.6f} {significance}\")\n",
        "            \n",
        "            if 'pairwise' in results and results['pairwise']:\n",
        "                print(f\"    Significant pairwise comparisons:\")\n",
        "                for pair, pair_result in results['pairwise'].items():\n",
        "                    if pair_result['significant']:\n",
        "                        nb1, nb2 = pair.split('_vs_')\n",
        "                        sig = '***' if pair_result['p_value'] < 0.001 else '**' if pair_result['p_value'] < 0.01 else '*'\n",
        "                        print(f\"      NB{nb1} vs NB{nb2}: p={pair_result['p_value']:.6f} {sig}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Trend Analysis\n",
        "\n",
        "Analyze performance trends as neighborhood size varies:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Trend analysis\n",
        "trend_df = analyzer.performance_trend_analysis()\n",
        "\n",
        "print(\"Trend Analysis:\")\n",
        "print(\"=\"*60)\n",
        "trend_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display improvements/degradations\n",
        "print(\"\\nImprovements from NB=3 to NB=7:\")\n",
        "print(\"=\"*60)\n",
        "improvements = trend_df[['Model Type', 'Metric', 'Improvement_3_to_7']].copy()\n",
        "improvements = improvements.dropna()\n",
        "improvements = improvements.sort_values('Improvement_3_to_7')\n",
        "\n",
        "for _, row in improvements.iterrows():\n",
        "    improvement = row['Improvement_3_to_7']\n",
        "    direction = \"IMPROVEMENT\" if improvement > 0 else \"DEGRADATION\"\n",
        "    print(f\"{row['Model Type']} - {row['Metric']}: {improvement:.2f}% ({direction})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Computational Complexity Analysis\n",
        "\n",
        "Measure the computational cost for each neighborhood size:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Computational complexity analysis\n",
        "complexity_df = analyzer.computational_complexity_analysis(n_samples=5)\n",
        "\n",
        "print(\"Computational Complexity:\")\n",
        "print(\"=\"*60)\n",
        "complexity_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display computational complexity\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=complexity_df['Neighborhood Size'],\n",
        "    y=complexity_df['Mean Time (s)'],\n",
        "    mode='lines+markers',\n",
        "    name='Mean time (s)',\n",
        "    error_y=dict(type='data', array=complexity_df['Std Time (s)'], visible=True),\n",
        "    line=dict(width=3, color='blue'),\n",
        "    marker=dict(size=12)\n",
        "))\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=complexity_df['Neighborhood Size'],\n",
        "    y=complexity_df['Normalized Time'],\n",
        "    mode='lines+markers',\n",
        "    name='Normalized time (vs NB=3)',\n",
        "    line=dict(width=3, color='red', dash='dash'),\n",
        "    marker=dict(size=12)\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Computational Complexity vs Neighborhood Size',\n",
        "    xaxis_title='Neighborhood Size',\n",
        "    yaxis_title='Time (s) / Normalization Factor',\n",
        "    width=1000,\n",
        "    height=600,\n",
        "    template='plotly_white',\n",
        "    hovermode='x unified'\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interactive Visualizations\n",
        "\n",
        "Create interactive visualizations with Plotly:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create all visualizations\n",
        "analyzer.create_visualizations()\n",
        "\n",
        "print(\"\\n Visualizations created! Check the analysis_plots/ folder\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Custom Visualizations in the Notebook\n",
        "\n",
        "Create interactive visualizations directly in the notebook:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive dashboard with all metrics\n",
        "metric_cols = ['KL Divergence', 'Chi-Square', 'Categorical MMD', \n",
        "              'Tumor Size Diff', 'Border Size Diff', 'Spatial Variance Diff']\n",
        "\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=3,\n",
        "    subplot_titles=metric_cols,\n",
        "    vertical_spacing=0.12,\n",
        "    horizontal_spacing=0.1\n",
        ")\n",
        "\n",
        "colors = px.colors.qualitative.Set2\n",
        "df_parsed = analyzer.parse_metrics()\n",
        "\n",
        "for idx, metric in enumerate(metric_cols):\n",
        "    if metric not in df_parsed.columns:\n",
        "        continue\n",
        "    \n",
        "    row = (idx // 3) + 1\n",
        "    col = (idx % 3) + 1\n",
        "    \n",
        "    for model_idx, model_type in enumerate(df_parsed['Model Type'].unique()):\n",
        "        model_data = df_parsed[df_parsed['Model Type'] == model_type]\n",
        "        grouped = model_data.groupby('Neighborhood Size')[metric].agg(['mean', 'std'])\n",
        "        \n",
        "        sizes = grouped.index.values\n",
        "        means = grouped['mean'].values\n",
        "        stds = grouped['std'].values\n",
        "        \n",
        "        color = colors[model_idx % len(colors)]\n",
        "        \n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                x=sizes,\n",
        "                y=means,\n",
        "                mode='lines+markers',\n",
        "                name=model_type if idx == 0 else '',\n",
        "                line=dict(color=color, width=2),\n",
        "                marker=dict(size=8, color=color),\n",
        "                error_y=dict(type='data', array=stds, visible=True),\n",
        "                showlegend=(idx == 0),\n",
        "                hovertemplate=f'<b>{model_type}</b><br>' +\n",
        "                            'Neighborhood Size: %{x}<br>' +\n",
        "                            f'{metric}: %{{y:.4f}}<br>' +\n",
        "                            '<extra></extra>'\n",
        "            ),\n",
        "            row=row, col=col\n",
        "        )\n",
        "\n",
        "fig.update_layout(\n",
        "    title_text=\"Complete Dashboard: Performance by Neighborhood Size\",\n",
        "    height=1000,\n",
        "    width=1800,\n",
        "    font=dict(size=10),\n",
        "    title_font_size=18,\n",
        "    template='plotly_white'\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive box plot for a specific metric\n",
        "metric = 'KL Divergence'  # Change this metric to explore others\n",
        "\n",
        "fig = px.box(\n",
        "    df_parsed, \n",
        "    x='Neighborhood Size', \n",
        "    y=metric, \n",
        "    color='Model Type',\n",
        "    title=f'{metric} by Neighborhood Size and Model Type',\n",
        "    labels={'Neighborhood Size': 'Neighborhood Size', metric: metric}\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    width=1200,\n",
        "    height=700,\n",
        "    font=dict(size=12),\n",
        "    title_font_size=16,\n",
        "    template='plotly_white'\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cost-Benefit Analysis\n",
        "\n",
        "Compare performance improvement with computational cost:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cost-benefit analysis: improvement vs complexity\n",
        "# For each model, calculate the relative improvement and compare it with the cost\n",
        "\n",
        "cost_benefit_analysis = []\n",
        "\n",
        "for model_type in df_parsed['Model Type'].unique():\n",
        "    model_data = df_parsed[df_parsed['Model Type'] == model_type]\n",
        "    \n",
        "    # Calculate average improvement across all metrics (normalized)\n",
        "    nb3_data = model_data[model_data['Neighborhood Size'] == 3]\n",
        "    nb7_data = model_data[model_data['Neighborhood Size'] == 7]\n",
        "    \n",
        "    if len(nb3_data) > 0 and len(nb7_data) > 0:\n",
        "        improvements = []\n",
        "        for metric in metric_cols:\n",
        "            if metric in model_data.columns:\n",
        "                mean3 = nb3_data[metric].mean()\n",
        "                mean7 = nb7_data[metric].mean()\n",
        "                if mean3 > 0:\n",
        "                    improvement = (mean3 - mean7) / mean3 * 100  # % improvement\n",
        "                    improvements.append(improvement)\n",
        "        \n",
        "        avg_improvement = np.mean(improvements) if improvements else 0\n",
        "        \n",
        "        # Computational complexity (normalized to NB=3)\n",
        "        complexity_nb7 = complexity_df[complexity_df['Neighborhood Size'] == 7]['Normalized Time'].values[0] if len(complexity_df[complexity_df['Neighborhood Size'] == 7]) > 0 else 1\n",
        "        \n",
        "        cost_benefit_analysis.append({\n",
        "            'Model Type': model_type,\n",
        "            'Avg Improvement (%)': avg_improvement,\n",
        "            'Computational Cost (x)': complexity_nb7,\n",
        "            'Efficiency (Improvement/Cost)': avg_improvement / complexity_nb7 if complexity_nb7 > 0 else 0\n",
        "        })\n",
        "\n",
        "cost_benefit_df = pd.DataFrame(cost_benefit_analysis)\n",
        "print(\"Cost-Benefit Analysis (NB=3 vs NB=7):\")\n",
        "print(\"=\"*60)\n",
        "cost_benefit_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display cost-benefit analysis\n",
        "fig = go.Figure()\n",
        "\n",
        "for _, row in cost_benefit_df.iterrows():\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=[row['Computational Cost (x)']],\n",
        "        y=[row['Avg Improvement (%)']],\n",
        "        mode='markers+text',\n",
        "        name=row['Model Type'],\n",
        "        marker=dict(size=15),\n",
        "        text=[row['Model Type']],\n",
        "        textposition=\"top center\",\n",
        "        hovertemplate=f\"<b>{row['Model Type']}</b><br>\" +\n",
        "                      f\"Improvement: {row['Avg Improvement (%)']:.2f}%<br>\" +\n",
        "                      f\"Cost: {row['Computational Cost (x)']:.2f}x<br>\" +\n",
        "                      f\"Efficiency: {row['Efficiency (Improvement/Cost)']:.2f}<br>\" +\n",
        "                      \"<extra></extra>\"\n",
        "    ))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Cost-Benefit Analysis: Improvement vs Computational Complexity',\n",
        "    xaxis_title='Computational Cost (normalized to NB=3)',\n",
        "    yaxis_title='Average Performance Improvement (%)',\n",
        "    width=1000,\n",
        "    height=700,\n",
        "    template='plotly_white',\n",
        "    hovermode='closest'\n",
        ")\n",
        "\n",
        "# Add reference lines\n",
        "fig.add_hline(y=0, line_dash=\"dash\", line_color=\"gray\", annotation_text=\"No improvement\")\n",
        "fig.add_vline(x=1, line_dash=\"dash\", line_color=\"gray\", annotation_text=\"Base cost (NB=3)\")\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Complete Report Generation\n",
        "\n",
        "Generate the complete textual report:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate complete report\n",
        "analyzer.generate_report()\n",
        "\n",
        "print(\"\\n Report generated! Check the file neighborhood_size_analysis_report.txt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusions and Recommendations\n",
        "\n",
        "Summary of main results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find the best configuration for each metric\n",
        "print(\"=\"*60)\n",
        "print(\"BEST CONFIGURATIONS BY METRIC\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for metric in metric_cols:\n",
        "    if metric not in df_parsed.columns:\n",
        "        continue\n",
        "    \n",
        "    best_idx = df_parsed[metric].idxmin()\n",
        "    best_row = df_parsed.loc[best_idx]\n",
        "    print(f\"\\n{metric}:\")\n",
        "    print(f\"  Best: {best_row['Model Type']} with NB={best_row['Neighborhood Size']}\")\n",
        "    print(f\"  Value: {best_row[metric]:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RECOMMENDATIONS\")\n",
        "print(\"=\"*60)\n",
        "print(\"\"\"\n",
        "1. Analyze statistical tests to determine if differences are significant\n",
        "2. Consider the trade-off between performance improvement and computational cost\n",
        "3. Verify if larger neighborhood sizes provide consistent improvements\n",
        "4. Evaluate if the improvement justifies the increase in computational cost\n",
        "5. Consider using a larger neighborhood size only if:\n",
        "   - Statistical tests show significant differences\n",
        "   - The improvement is consistent across all metrics\n",
        "   - The computational cost is acceptable for your use case\n",
        "\"\"\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
